{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on 03 - Modelagem e Resolução de um Processo de Decisão de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Hands-on é destinado a modelagem do processo de decisão de Markov, bem como a aplicação da iteração por Q-valor na sua resolução.\n",
    "\n",
    "Na figura abaixo se encontra a PDM (Processo de Decisão de Markov) que será utilizada como problema para aplicar a resolução via Q-valor:\n",
    "\n",
    "![alt text](rlimages/mdp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está MDP é composta de 3 estados, sendo o número de ações por estado variável. No estado S0 pode-se tomar 3 ações diferentes. Já estado S1 tem-se duas ações possíveis e no estado S2 apenas uma.\n",
    "\n",
    "Nota-se que para cada ação tomada há probabilidades de transição associadas. No estado S0, caso a ação tomada seja a a0, há 70% de chance de o agente continuar no mesmo estado S0 ganhando uma recompensa de +10, enquanto há 30% dele ir para o estado S1 sem recompensa associada.\n",
    "\n",
    "No estado S1 há duas ações possíveis: a0 para continuar no mesmo estado e não ganhar nada, ou a2 onde o agente passa ao estado S2 e é penalizado com uma recompensa de -50.\n",
    "\n",
    "No estado S2 há apenas uma ação que pode ser tomada e ela pode levar, com 80% de chance, o agente a ganhar +40 de recompensa. \n",
    "\n",
    "***\n",
    "\n",
    "**Você conseguiria indicar qual a melhor estratégia, i.e melhor conjunto de ações, que o agente deveria tomar?** No estado S0 fica claro que a melhor ação é a a0, pois ela tem 70% de chance de recompensar o agente com +10. No estado S2 não há problemas na escolha da melhor ação, pois há somente uma ação a tomar. Já no estado s1 há duas ações, uma que o agente fica indeterminadamente no mesmo estado ou outra que ele é penalizado, porém não é tão claro, como nos demais estados, seria seria a melhor ação que ele deveria tomar: continuar no estado ou ir para o próximo e ser penalizado.\n",
    "\n",
    "Para responder esse questionamento, vamos aplicar a iteração por Q-valor e determinar não somente a função Q ótima como também a melhor policy para esse PDM.\n",
    "\n",
    "O código fica asism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nan = np.nan # representa acoes impossiveis (não se pode ir direto do estado S0 para o S2 com uma determinada acao)\n",
    "\n",
    "'''\n",
    "probabilidades de transicao por cada acao tomada:\n",
    "\n",
    "linhas -> estado atual do agente. Pode ser 0, 1 ou 2\n",
    "\n",
    "colunas -> acao tomada pela agente. Ha tres opcoes, mas nem todo os estados possuem todas as acoes, assim alguns tem 'nan'\n",
    "como valor para indicar essa impossibilidade\n",
    "\n",
    "O valor T[s, a] é um vetor de 3 elementos em que os indices indicam o proximo estado e o valor a probabilidade de transicao\n",
    "'''\n",
    "T = np.array([[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],  # shape [s, a, s']\n",
    "            [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "            [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]]])\n",
    "\n",
    "#recompensas por acoes tomadas em cada estado. Mesmo esquema que a matriz T\n",
    "R = np.array([[[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], # shape [s, a, s']\n",
    "            [[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "            [[nan, nan, nan], [40, 0.0, 0.0], [nan, nan, nan]]])\n",
    "\n",
    "act_possiveis = [[0, 1, 2],[0, 2],[1]] # acoes possiveis por cada estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.88646117 20.79149867 16.854807  ]\n",
      " [ 1.10804034        -inf  1.16703135]\n",
      " [       -inf 53.8607061         -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.full((3, 3), -np.inf) # matriz Q que relaciona o valor Q de cada par s, a \n",
    "\n",
    "for estado, act in enumerate(act_possiveis): # inicializar os valores da matriz em zero para as situacoes possiveis\n",
    "    Q[estado, act] = 0.0\n",
    "    \n",
    "learning_rate  = 0.01\n",
    "gamma = 0.95 \n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    Q_prev = Q.copy() # uma copia para ser utilizada na atualizacao da matriz Q\n",
    "    \n",
    "    for s in range(3):\n",
    "        for a in act_possiveis[s]: #para cada estado e para cada acao atualiza a matriz Q de acordo com a equacao\n",
    "            Q[s, a] = np.sum([T[s, a, sp] * (R[s, a, sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])\n",
    "            \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos a matriz Q ótima depois do processo de iteração, vamor definir a melhor *policy* do agente para este processo de decisão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_otima = np.argmax(Q, axis=1)\n",
    "policy_otima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "nota-se que a policy dada pela matriz Q pós-iteração indica o seguinte mapeamento: Para o estado S0, a melhor ação é ação 0, isto é, a ação a0. Para o estado S1, a melhor ação é a a2, e para o estado S2 a melhor ação é a a1.\n",
    "****\n",
    "\n",
    "Exposto tudo isso, vamos colocar a mão na massa agora. Faça o que se pede:\n",
    "    1. Crie um processo de decisão de Markov com no mínimo 5 estados\n",
    "    2. Para cada estado defina as ações possíveis\n",
    "    3. Defina as probabilidades de transição para cada ação\n",
    "    5. Use a iteração por Q-valor para resolver essa PDM criada\n",
    "\n",
    "OBS: use a PDM do exemplo deste Hands-on como base para a PDM que você vai criar. Use também o código como base para fazer as devidas modificações (caso seja necessário). Recomenda-se também que seja desenhado o processo de decisão criado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

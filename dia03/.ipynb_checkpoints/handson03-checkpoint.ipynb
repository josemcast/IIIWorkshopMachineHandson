{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 3  - Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este Hands-on vamos mudar um pouco. Nos hands-on passados fizemos toda a modelagem para o ambiente, desde os estados até as probabilidades de transição. Como o foco deste hands-on é o Q-learning, vamos usar um framework para lidar com toda a parte do ambiente e seus elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym é um conjunto de ferramentos e framework para lidar com problemas de aprendizagem por reforço. Ele foi criado devido a necessidade de padronização nos testes de algoritmos e de *benchmarks*. Assim, esse framework lida com toda a parte da relação agente-ambiente-ações-recompensas e o desenvolvedor fica livre para focar apenas no algoritmo de RL (Reinforcement Learning) que irá usar. Para mais informações sobre o gym, visite o site: [gym website](https://gym.openai.com/)\n",
    "\n",
    "Para começar, vamos importar o pacote do gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gym possui vários ambientes para testes de algoritmos de aprendizado por reforço. Alguns desses ambientes são bem simples, enquanto que outros são bem mais complexos. Para este hands-on vamos usar o ambiente \"Taxi-v2\". Para conhecer os demais ambientes visite [gym ambientes](https://gym.openai.com/envs/#classic_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ambiente consiste num grid com quatro posições padrões: R, G Y, B. Sempre que o ambiente é inicializado, uma dessas posições recebe a cor azul e indica onde está um passageiro. Outra posição recebe a cor magenta e indica o destino que se deve levar o passageiro. O táxi é representado pela cor amarela quando está vázio e pela cor verde quando está com um passageiro. Abaixo um exemplo de como é o grid:\n",
    "\n",
    "   ![alt text](rlimages/taxi01.png)\n",
    "   \n",
    "no grid há posições onde o taxí pode ir, e as ações que ele pode escolher são: Norte, Sul , Leste, Oeste, Pickup (pegar o passageiro) e Dropoff (liberar o passageiro). Cada movimento que o táxi realiza ele é penalizado em -1. Caso ele pegue o passageiro no local certo e entregue no local também correto, ele recebe uma recompensa de +20. Porém, caso o táxi faça um pickup ou dropoff fora do local certo ele é penalizado em -10. Vamos rodar um simulação para mostrar a interação do agente com ambiente com escolhas de ações aleatórias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(1):\n",
    "\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render() # renderizar o ambiente grafico do ambiente\n",
    "        action = env.action_space.sample() # funcao do gym para escolha de acao aleatoria dado o ambiente\n",
    "        observation, reward, done, info = env.step(action) # aplicacao do acao e retorno o proximo estado, recompensa e demais infos\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timestep\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note que certas vezes o táxi realiza um pickup ou dropoff fora do local indicado. E que a simulação acaba não por ter conseguido o objetivo, mas por causa do limite do número máximo de iterações permitada caso o agente não consiga realizar o objetivo. \n",
    "\n",
    "Vale o destaque para as funções **env.step()** e **env.reset()**. A função **env.step()** recebe como argumento a ação tomada e retorna o próximo estado (ou observação), a recompensa, um variável *done* do tipo *bool* que indica se o episódio chegou ao final e uma outra variável denominada info, que para nosso hands-on não será utilizada. a função **env.reset()** deve ser sempre utilizada após a definição do ambiente e **retorna o estado inicial do ambiente** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Para aplicar o Q-Learning nesse framework a primeira coisa que necessitamos e saber a quantidade de estados que esse ambiente possui e a quantidade de ações. Para isso, basta fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota-se que há 6 possíveis ações que podem ser tomadas nesse ambiente e que ele possui 500 estados. Nota-se também que o tipo dessas variáveis é *Discrete* que indica que os valores são inteiros (o que é bom, pois não teremos problemas com indexação na matrix Q necessária no Q-learning). \n",
    "\n",
    "Conhecendo esses valores podemos agora aplicar o Q-Learning no processo de aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) # Matrix Q necessaria no Q-Learning\n",
    "\n",
    "G = 0 # variavel para acumular as recompensas\n",
    "\n",
    "alpha = 0.618 # learning rate \n",
    "\n",
    "for episode in range(1, 10001):  #1000 episodios de aprendizagem\n",
    "\n",
    "    done = False\n",
    "    G, reward = 0, 0\n",
    "    state = env.reset() # estado inicial\n",
    "\n",
    "    while done != True:\n",
    "\n",
    "        action = np.argmax(Q[state]) # escolha a acao\n",
    "        state2, reward, done, info = env.step(action) # aplique a acao no ambiente e receba as observacoes e recompensa\n",
    "        Q[state, action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) # Q-learning \n",
    "        G += reward # acumular a recompensa\n",
    "        state = state2 # mudando para o novo estado\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {} total reward: {}'.format(episode, G)) # a cada 50 episodios imprime informacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Espere até o código acima termine de rodar (aparecerá diversas informações logo abaixo dele assim que ele terminar)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o código acima executado, nosso agente (o táxi) já aprendeu a dinâmica do ambiente e já está pronto para executar a melhor ação dado cada estado. Para testar, vamos rodar o seguinte código dado a matrix Q já definida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "G = 0\n",
    "env.render()\n",
    "while done != True:\n",
    "\n",
    "    action = np.argmax(Q[state])\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    G += reward\n",
    "    state = state2\n",
    "\n",
    "print('Accumulative Reward: {}'.format(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, notamos que o agente consegue executar o objetivo dado a matrix Q e estado inicial. **Análise a saída do código anterior e certifique-se que de fato o agente está fazendo o objetivo.**\n",
    "\n",
    "****\n",
    "\n",
    "Agora, vamos colocar a mão na massa um pouco. Para isso, vamos utilizar um ambiente diferente. Vamos usar o ambiente \"CartPole-v0\". Veja no link como é esse ambiente e como ele funciona: [CartPole ](https://gym.openai.com/envs/CartPole-v1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que o objetivo nesse ambiente e fazer com que o carro consiga equilibrar o poste. E que para isso ele pode se mover para direita ou esquerda, bem como modificar a velocidade nos dois sentidos. \n",
    "\n",
    "Exposto isso, vamos seguir e observar agora quais são os estados e ações desse ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui temos um pequeno problema. Os estados nesse ambiente são do tipo *box(4,)*, e isso indica que cada estado é representado por um vetor com 4 elementos que indicam medidas. Essas medidas são apresentadas na figura abaixo\n",
    "\n",
    "![alt text](rlimages/medidas.png)\n",
    "\n",
    "\n",
    "E esse não é o único problema, se fizermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notamos que os valores não são inteiros para esses elementos, o que indica que não será possível usar esse array como indexação na matrix Q. Assim, temos que fazer uma discretização para poder utilizar esse ambiente. \n",
    "\n",
    "Abaixo é definida uma função para discretizar as observações (estados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "buckets=(1, 1, 6, 12)\n",
    "def discretize(obs):\n",
    "\n",
    "    upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "    lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "    ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "    new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "    new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "    \n",
    "    return tuple(new_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função será utilizada sempre que tivermos um estado retornado do ambiente e antes de utilizá-lo para o cálculo do valor Q na matrix.\n",
    "\n",
    "Assim, o que se pede é: Utilizando como base o código no exemplo do Táxi. Faça com que o agente consiga o objetivo nesse ambiente do \"CartPole\".\n",
    "\n",
    "Para auxiliar, abaixo se encontra um esbouço do código. Complete com o que falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros(buckets + (env.action_space.n,))\n",
    "\n",
    "G = 0 # variavel para acumular as recompensas\n",
    "\n",
    "alpha = 0.1 \n",
    "gamma = 1.0\n",
    "\n",
    "n_episodios = ?\n",
    "for episode in range(1, n_episodios+1):  \n",
    "\n",
    "    done = False\n",
    "    G, reward = 0, 0\n",
    "    current_state = discretize(env.reset())\n",
    "\n",
    "    while done != True:\n",
    "\n",
    "        action = np.argmax()\n",
    "        state2, ?, ?, ? = env.step(action)\n",
    "        state2 = discretize(state2)\n",
    "        ?\n",
    "        ?\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {} total reward: {}'.format(episode, G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar se o agente de fato aprendeu a dinâmica do ambiente, rode o seguinte código e veja o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "G = 0\n",
    "env.render()\n",
    "n = 0\n",
    "while (done != True):\n",
    "    current_state = discretize(env.reset())\n",
    "    action = np.argmax(Q[current_state])\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    G += reward\n",
    "    current_state = state2\n",
    "    n += 1\n",
    "    if n == 1000: done = True\n",
    "\n",
    "env.reset()\n",
    "env.close()\n",
    "print('Accumulative Reward: {}'.format(G))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

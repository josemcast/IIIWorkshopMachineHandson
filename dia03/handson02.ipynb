{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands - On 2 - Programação Dinâmica com iteração por Policy ou Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este hands-on é destinado a aplicação da solução via programação dinâmica das equações de Bellman para um processo de decisão de Markov. O ambiente que será usado para a solução é o *grid world* como apresentado abaixo\n",
    "\n",
    "![alt text](rlimages/grid01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nosso agente será um robô que será inserido neste ambiente e que o processo de aprendizagem (episódio) acaba que o robô alcança o estado (4,3), com recompensa +1, e o estado (4,2), com recompensa -1. Os valores de -0.02 nos demais estados são custos que representam o custo associado ao movimento do robô (consumo de bateria). O objetivo é fazer o robô chegar ao estado (4,3) com o menor custo possível (maior recompensa).  \n",
    "***\n",
    "Relembrando, a solução via programação dinâmica pode ocorrer de dois modos: **Iteração por valor** ou **Iteração por Policy**.\n",
    "\n",
    "Na **iteração por valor** segue o seguinte algoritmo:\n",
    "*******\n",
    "1. Inicialize V(s) = 0 para todos os estados s\n",
    "2. Para todo s, atualize:\n",
    "    ![alt text](rlimages/value_ite.png)  \n",
    "3. Repetidamente computando o passo 2, haverá convergência para o valor ótimo de cada estado:\n",
    "    ![alt text](rlimages/value_conv.png)\n",
    "    \n",
    "ao fim do processo de iteração o agente terá aprendido a dinâmica do ambiente e poderá realizar sempre a melhor ação dado o estado que ele se encontra. Para isso, basta a partir do valor final de V(s) montar a policy ótima que é dado como:\n",
    "    ![alt text](rlimages/policy_value.png)\n",
    "*****\n",
    "\n",
    "Já na **iteração por policy** segue o seguinte algoritmo:\n",
    "*******\n",
    "1. Inicialize uma policy aleatória ∏\n",
    "2. Repetidamente faça o seguinte até a convergência:\n",
    "    Resolva a equação de Bellman para o policy atual\n",
    "    ![alt text](rlimages/policy.png)  \n",
    "3. Calcule a policy como se fosse a ótima para a função valor V(s) calculada:\n",
    "    ![alt text](rlimages/policy_value.png)\n",
    "4. Repita os passos e ambos, o valor e policy, irão convergir para o valor ótimo\n",
    "    ![alt text](rlimages/policy_conv.png)\n",
    "    \n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrarmos de fato na código da resolução, vamos relembrar que usaremos a seguinte dinâmica para as probabilidades de transição:\n",
    "    ![alt text](rlimages/dinamica.png)\n",
    "\n",
    "essa dinâmica indica que cada vez que o agente escolhe uma certa ação, por exemplo ir para o norte, Há 80% de chances dele ir de fato para o norte, porém há 10% de chance dele ir para o leste (direita) ou 10% dele ir para o oeste (esquerda).\n",
    "\n",
    "Exposto isso, vamos à aplicação da solução via programação dinâmica. Primeiramente vamos definir algumas funções auxiliares para a movimentação do agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, operator\n",
    "\n",
    "def argmax(seq, fn): \n",
    "\n",
    "    best = seq[0]; best_score = fn(best)\n",
    "\n",
    "    for x in seq:\n",
    "        x_score = fn(x)\n",
    "        if x_score > best_score:\n",
    "            best, best_score = x, x_score\n",
    "    return best\n",
    "\n",
    "def vector_add(a, b): #adicao sem precisar da numpy\n",
    "\n",
    "    return tuple(map(operator.add, a, b))\n",
    "\n",
    "\n",
    "#orientacoes: leste, norte, oeste, sul\n",
    "orientations = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "\n",
    "def turn_right(orientation):\n",
    "\n",
    "    return orientations[orientations.index(orientation)-1]\n",
    "\n",
    "def turn_left(orientation):\n",
    "    \n",
    "    return orientations[ (orientations.index(orientation)+1) % len(orientations)]\t\n",
    "\n",
    "def isnumber(x): \n",
    "    return hasattr(x, '__int__')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com as funções auxiliares criadas, vamos agora criar uma classe que representará a MDP (Markov Decision process - Processo de Decisão Markov) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    def __init__(self, init_pos, actlist, terminals, transitions={}, states=None, gamma=0.99):\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('MDP deve ter 0 < gamma <=1')\n",
    "        if states:\n",
    "            self.states = states # estados\n",
    "        else:\n",
    "            self.states = set() \n",
    "        \n",
    "        self.init_pos = init_pos # posicao inicial\n",
    "        self.actlist = actlist # lista de acoes\n",
    "        self.terminals = terminals # Estado final - objetivo\n",
    "        self.transitions = transitions # transicoes de estados\n",
    "        self.gamma = gamma # gamma\n",
    "        self.reward = {} # recompensa \n",
    "\n",
    "    def R(self, state): #retorna a recompensa dado o estado\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action): #retorna a a prob. de transicao dado a acao e o estado atual\n",
    "\n",
    "        if(self.transitions == {}):\n",
    "\n",
    "            raise ValueError(\"Modelo de transicao nao definido\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state): #retorna a acao dado o estado\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora definimos um classe para representar o grid que é o ambiente estará inserido. Essa classe do grid será derivada da classe MDP, pois como já se sabe o ambiente e seus estados são elementos da processo de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDP(MDP):\n",
    "\n",
    "    def __init__(self, grid, terminals, init_pos=(0,0), gamma = 0.99):\n",
    "\n",
    "        grid.reverse() # reverte os indices para ficar de acordo com a figura do grid apresentado no começo\n",
    "\n",
    "        #inicializacao do grid\n",
    "        MDP.__init__(self, init_pos, actlist=orientations, terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x,y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add( (x, y) )\n",
    "\n",
    "    def T(self, state, action):\n",
    "\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "\n",
    "            return [(0.8, self.go(state,action)), \n",
    "                    (0.1, self.go(state, turn_right(action))), \n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction): #retorna o proximo estado dado o estado atual e direcao indicada pela acaoo\n",
    "\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping): #auxiliar para o grid sem peso para o problema em si\n",
    "\n",
    "        return list(reversed([[mapping.get((x,y), None) for x in range(self.cols)] for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy): # retorna o grid com arrows indicando a policy ótima\n",
    "        chars = { (1,0): '>', (0,1): '^', (-1,0): '<', (0,-1): 'v', None:'.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos agora definir as funções para resolução por valor e por policy bem como algumas outras funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon=0.001): #iteração por valor - primeira opcao de resolucao\n",
    "\n",
    "    STSN = {s: 0 for s in mdp.states}\n",
    "\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "\n",
    "    while True:\n",
    "            \n",
    "        STS = STSN.copy()\n",
    "        delta = 0\n",
    "\n",
    "        for s in mdp.states:\n",
    "\n",
    "            STSN[s] = R(s) + gamma * max([sum([p * STS[s1] for (p, s1) in T(s, a)]) for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(STSN[s] - STS[s]))\n",
    "\n",
    "            if delta < \tepsilon * (1 - gamma)/ gamma:\n",
    "                return STS\n",
    "\n",
    "def best_policy(mdp, STS): #retorna a melhor policy dado a MDP e a função valor\n",
    "\n",
    "    pi = {}\n",
    "\n",
    "    for s in mdp.states:\n",
    "\n",
    "        pi[s] = argmax(mdp.actions(s), lambda a: expected_utility(a, s, STS, mdp))\n",
    "\n",
    "    return pi\n",
    "\n",
    "def expected_utility(a, s, STS, mdp): #funcao para calculo da policy otima dado a funcao valor já definida\n",
    "\n",
    "    return sum([p * STS[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "def policy_iteration(mdp): #resolucao por policy\n",
    "\n",
    "    STS = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        STS = policy_evaluation(pi, STS, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "\n",
    "            a = argmax(mdp.actions(s), lambda a: expected_utility(a, s, STS, mdp))\n",
    "\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "\n",
    "        if unchanged:\n",
    "            return pi \n",
    "\n",
    "def policy_evaluation(pi, STS, mdp, k = 20): #dado a policy resolve o sistema via iteração por valor (resolucao por Belmman)\n",
    "\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            STS[s] = R(s) + gamma * sum([p * STS[s1] for (p, s1) in T(s, pi[s])])\n",
    "\n",
    "    return STS\n",
    "\n",
    "def print_table(table, header = None, sep = '   ', numftm = '{}'): #auxiliar para imprimir o grid no console - Não importa para a resolução em si\n",
    "\n",
    "\n",
    "    justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]] \n",
    "    if header:\n",
    "\n",
    "        table.insert(0, header)\n",
    "\n",
    "    table = [[numftm.format(x) if isnumber(x) else x for x in row] for row in table]\n",
    "\n",
    "    sizes = list(map(lambda seq: max(map(len, seq)), list(zip(*[map(str, row) for row in table ]))))\n",
    "\n",
    "    for row in table:\n",
    "        print(sep.join(getattr(str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy  based on  Value iteration \n",
      "\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   <      <   <\n",
      "\n",
      "\n",
      "Optimal policy  based on  policy iteration \n",
      "\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   <      <   <\n"
     ]
    }
   ],
   "source": [
    "env_grid = [[-0.02, -0.02, -0.02, 1],\n",
    "                [-0.02, None, -0.02, -1],\n",
    "                [-0.02, -0.02, -0.02, -0.02]] # definicao do grid do ambiente\n",
    "\n",
    "terminals = [(3,2), (3, 1)] # estados onde pode ocorrer o encerramento do episodio\n",
    "\n",
    "sequential_dec_env = GridMDP(env_grid, terminals=terminals) \n",
    "\n",
    "epsilon = 0.01 #definir um limiar para convergencia\n",
    "\n",
    "vl = value_iteration(sequential_dec_env, epsilon) # resolucao por valor\n",
    "value_iter = best_policy(sequential_dec_env, vl) # best policy dado o valor ótimo\n",
    "print('Optimal policy  based on  Value iteration \\n')\n",
    "print_table(sequential_dec_env.to_arrows(value_iter))\n",
    "\n",
    "print('\\n')\n",
    "policy_iter = policy_iteration(sequential_dec_env) #resolucao por policy\n",
    "print('Optimal policy  based on  policy iteration \\n')\n",
    "print_table(sequential_dec_env.to_arrows(policy_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Agora, tire um tempo para enter bem como este código funciona. Foque nas funções da iteração por valor e iteração por policy. Depois de entendido a base de funcionamento faça o seguinte:\n",
    "\n",
    "   1. Rode problema para alguns valores diferentes de gamma. Teste gamma com valores próximos de 0 e com valores próximos de 1\n",
    "         - O que acontece com a policy ótima ao final de cada modificação? Por que acontece isso com a policy? \n",
    "   2. Rode o problema para alguns valores diferentes de epsilon. \n",
    "       - O que acontece com a policy ótima ao final de cada modificação? Por que acontece isso com a policy?\n",
    "   3. Bônus - Desafio:\n",
    "       - Quantas iterações são necessárias para a convergência na iteração por valor? Crie um lógica dentro da função value_iteration() para fazer essa contagem. Teste para vários valores de epsilon e gamma\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

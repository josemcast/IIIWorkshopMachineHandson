{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição do problema\n",
    "\n",
    "Usuários da eNB1 se afastam da mesma em linha reta e com um ângulo aleatório, enquanto realizam o download de um vídeo de 15 MB. Eventualmente, devido ao enfraquecimento do sinal da sua eNB, eles fazem handover para a eNB2 ou eNB3 através do algoritmo A3RSRP. Com acesso aos **sinais de referência** das 3 eNBs no momento do handover, use redes neurais para prever em qual **região** (region) se encontra cada usuário e qual **vazão** (throughput) possuem os mesmos nos primeiros 100 segundos de simulação.\n",
    "<img src=\"ang.png\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Pré-processamento de dados\n",
    "\n",
    "<font color=blue>Nesta etapa do HandsOn, você irá pré-processar os dados coletados de forma que os mesmos estejam adequados à utilização por redes neurais.</font>\n",
    "\n",
    "## Importando o dataset e as bibliotecas essenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nRun</th>\n",
       "      <th>rsrp1</th>\n",
       "      <th>rsrq1</th>\n",
       "      <th>rsrp2</th>\n",
       "      <th>rsrq2</th>\n",
       "      <th>rsrp3</th>\n",
       "      <th>rsrq3</th>\n",
       "      <th>region</th>\n",
       "      <th>throughput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-97.9726</td>\n",
       "      <td>-9.52930</td>\n",
       "      <td>-97.1633</td>\n",
       "      <td>-8.72002</td>\n",
       "      <td>-94.3961</td>\n",
       "      <td>-5.95282</td>\n",
       "      <td>superior</td>\n",
       "      <td>3.956754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-96.3151</td>\n",
       "      <td>-8.53851</td>\n",
       "      <td>-92.5126</td>\n",
       "      <td>-4.73598</td>\n",
       "      <td>-104.0420</td>\n",
       "      <td>-16.26500</td>\n",
       "      <td>inferior</td>\n",
       "      <td>3.950170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-96.7161</td>\n",
       "      <td>-8.88537</td>\n",
       "      <td>-92.9135</td>\n",
       "      <td>-5.08275</td>\n",
       "      <td>-100.0370</td>\n",
       "      <td>-12.20660</td>\n",
       "      <td>central</td>\n",
       "      <td>3.049065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-96.9771</td>\n",
       "      <td>-9.01505</td>\n",
       "      <td>-93.1471</td>\n",
       "      <td>-5.18508</td>\n",
       "      <td>-99.4380</td>\n",
       "      <td>-11.47600</td>\n",
       "      <td>central</td>\n",
       "      <td>2.912435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-96.2499</td>\n",
       "      <td>-8.68470</td>\n",
       "      <td>-92.2736</td>\n",
       "      <td>-4.70835</td>\n",
       "      <td>-103.3940</td>\n",
       "      <td>-15.82840</td>\n",
       "      <td>inferior</td>\n",
       "      <td>3.803656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nRun    rsrp1    rsrq1    rsrp2    rsrq2     rsrp3     rsrq3    region  \\\n",
       "0     0 -97.9726 -9.52930 -97.1633 -8.72002  -94.3961  -5.95282  superior   \n",
       "1     1 -96.3151 -8.53851 -92.5126 -4.73598 -104.0420 -16.26500  inferior   \n",
       "2     2 -96.7161 -8.88537 -92.9135 -5.08275 -100.0370 -12.20660   central   \n",
       "3     3 -96.9771 -9.01505 -93.1471 -5.18508  -99.4380 -11.47600   central   \n",
       "4     4 -96.2499 -8.68470 -92.2736 -4.70835 -103.3940 -15.82840  inferior   \n",
       "\n",
       "   throughput  \n",
       "0    3.956754  \n",
       "1    3.950170  \n",
       "2    3.049065  \n",
       "3    2.912435  \n",
       "4    3.803656  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Biblioteca usada para criar o frame de dados\n",
    "import pandas as pd\n",
    "# Importando a biblioteca base do python\n",
    "import numpy as np\n",
    "\n",
    "# Ignorando warnings (opcional)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importando os dados\n",
    "data=pd.read_csv('data.txt', delimiter='\\t')\n",
    "\n",
    "# Visualizando as primeiras linhas da variável data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: O **throughput** se encontra em Mbps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo os dados em variáveis independentes e dependente\n",
    "* A primeira tarefa a ser desempenhada pela rede neural será tentar prever em qual região o usuário que gerou esses dados está. Desta forma, *region* será a variável dependente.\n",
    "* Deve-se tentar identificar quais variáveis independentes influenciam no valor da região, pois apenas estas variáveis devem ser passadas para a rede.\n",
    "   * Resposta: os valores dos sinais de referência que ajudam a localizar a posiçao do usuário em relacao às 3 eNBs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variáveis independentes\n",
      "     rsrp1    rsrq1    rsrp2    rsrq2     rsrp3     rsrq3\n",
      "0 -97.9726 -9.52930 -97.1633 -8.72002  -94.3961  -5.95282\n",
      "1 -96.3151 -8.53851 -92.5126 -4.73598 -104.0420 -16.26500\n",
      "2 -96.7161 -8.88537 -92.9135 -5.08275 -100.0370 -12.20660\n",
      "3 -96.9771 -9.01505 -93.1471 -5.18508  -99.4380 -11.47600\n",
      "4 -96.2499 -8.68470 -92.2736 -4.70835 -103.3940 -15.82840\n",
      "\n",
      "Variável dependente\n",
      "0    superior\n",
      "1    inferior\n",
      "2     central\n",
      "3     central\n",
      "4    inferior\n",
      "Name: region, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Variáveis independentes\n",
    "x=data.iloc[:,1:7]\n",
    "print('Variáveis independentes\\n'+str(x.head())+'\\n')\n",
    "# Variável dependente\n",
    "y=data.iloc[:,7]\n",
    "print('Variável dependente\\n'+str(y.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com variáveis categóricas\n",
    "* No quadro acima, a variável *region* possui 3 categorias: superior, inferior e central. \n",
    "* O algoritmo não saberá lidar com essa variável em formato textual, portanto, é necessário expressá-la em números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importando a biblioteca usada para codificar variáveis categóricas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Criando um objeto Label Encoder\n",
    "label_encoder=LabelEncoder()\n",
    "\n",
    "# Transformando os valores da varável region\n",
    "y=label_encoder.fit_transform(y)\n",
    "\n",
    "# Resultado\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis categóricas com mais de 2 categorias e sem distição de maior-menor \n",
    "* A variável *region* tem agora suas categorias indicadas pelos números 0, 1 e 2. \n",
    "* Isto pode levar errôneamente a maquina a inferir que a região superior tem um valor maior que a central, que por sua vez, tem um valor maior do que a inferior, atrapalhado o processamento dos dados.\n",
    "* Para evitar este erro, devemos usar variáveis \"burras\" (**dummy variables**).\n",
    "* Isto não acontece quando há apenas 2 categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Importando a biblioteca usada para codificar as variáveis (já numericamente categorizadas) em dummy variables \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Alterando a forma de y para várias linhas e 1 coluna\n",
    "y=y.reshape(-1, 1)\n",
    "\n",
    "# Criando um objeto One Hot Encoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "y = onehotencoder.fit_transform(y).toarray()\n",
    "\n",
    "# Resultado\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo os dados em conjunto de treino e conjunto de testes\n",
    "* Antes de estar pronta para fazer suas previsões, a rede precisa passar por um **processo de aprendizagem** para se ajustar ao problema em questão. Neste caso, nossa rede aprenderá de maneira **supervisionada**, o que significa que forneceremos vários exemplos (**conjunto de treino**) com as respostas esperadas para que a rede **ajuste seus pesos**.\n",
    "* Após o processo de aprendizagem, podemos usar a rede para prever a informação desejada. Usualmente, testamos a eficácia da rede em um **conjunto de testes**, que é composto por exemplos que não estão presentes no conjunto de treino. Neste estágio, a rede apenas usa o que foi aprendido no anterior, sem reajustar seus pesos. As respostas esperadas não são, portanto, passadas à rede, sendo apenas usadas para medir a performance da mesma.\n",
    "* A biblioteca sklearn possui uma função que divide os dados da maneira mais eficaz possível, garantindo que os conjuntos de treino e testes possuam, ambos, amostras que representem de forma satisfatória todo o dataset. Já imaginou o que aconteceria se todos os exemplos passados à rede na etapa de treino fossem da mesma região? A rede jamais aprenderia a identificar as demais e sua performance seria bem limitada. Da mesma forma, não teríamos uma dimensao exata da precisão da rede se a testássemos apenas para uma região. Por isso, é importante usar a função ***train_test_split***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a funcao train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Obtendo os conjuntos de treino (80%) e testes (20%)\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composicao do conjunto de treino:\n",
      "Regiao inferior = 2598.0\n",
      "Regiao central = 2706.0\n",
      "Regiao superior = 2696.0\n",
      "\n",
      "Composicao do conjunto de testes:\n",
      "Regiao inferior = 675.0\n",
      "Regiao central = 645.0\n",
      "Regiao superior = 680.0\n"
     ]
    }
   ],
   "source": [
    "# Verificando a quantidade de dados de cada regiao\n",
    "\n",
    "# Conjunto de treino\n",
    "inf=y_train[:,0].sum()\n",
    "cent=y_train[:,1].sum()\n",
    "sup=y_train[:,2].sum()\n",
    "print('Composicao do conjunto de treino:')\n",
    "print('Regiao inferior = '+str(inf))\n",
    "print('Regiao central = '+str(cent))\n",
    "print('Regiao superior = '+str(sup) + '\\n')\n",
    "\n",
    "# Conjunto de testes\n",
    "inf=y_test[:,0].sum()\n",
    "cent=y_test[:,1].sum()\n",
    "sup=y_test[:,2].sum()\n",
    "print('Composicao do conjunto de testes:')\n",
    "print('Regiao inferior = '+str(inf))\n",
    "print('Regiao central = '+str(cent))\n",
    "print('Regiao superior = '+str(sup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "* É importante deixar os **features** (variáveis independentes) na **mesma escala** para acelerar o processamento da rede neural e melhorar o seu desempenho.\n",
    "* Existem vários tipos de padronizadores de escala para python, usaremos aqui o StandardScaler e o MinMaxScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *StandardScaler*\n",
    "* Deixa cada feature de x com uma distribuição gaussiana centrada em 0 e com desvio padrão igual a 1.\n",
    "* Fórmula:   \n",
    "   #### <center> $ x_{stand} = \\frac{x - media(x)}{std(x)}$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.16792464  1.03977203  0.8876307   0.7611939  -0.98160775 -1.03671221]\n",
      " [ 0.63633682  0.40519077  0.81193876  0.72903243 -0.60431154 -0.64843178]\n",
      " [ 0.90296154  1.38392602  0.78732034  0.79062354 -1.51098648 -1.45141723]\n",
      " [-0.41584662 -0.22320138 -1.29457875 -1.26367518  1.19815723  1.19007538]]\n"
     ]
    }
   ],
   "source": [
    "# Importando a biblioteca StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Criando um objeto standard scaler\n",
    "fs = StandardScaler()\n",
    "\n",
    "# Ajustando fs ao conjunto de treino\n",
    "# transformando os features do conjunto de treino\n",
    "x_train = fs.fit_transform(x_train)\n",
    "\n",
    "# Transformando os features do conjunto de testes\n",
    "# Só é necessário usar \"fit\" no conjunto de treino\n",
    "x_test = fs.transform(x_test)\n",
    "\n",
    "# Resultado\n",
    "print(x_test[0:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *MinMaxScaler*\n",
    "* Também é conhecido como min-max normalizer.\n",
    "* Deixa cada feature de x com valores percentuais na forma decimal, sendo min(x) = 0 e max(x) = 1.\n",
    "* Fórmula:  \n",
    "    ####  <center>  $ x_{norm} = \\frac{x - min(x)}{max(x) - min(x)}$  </center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "Realize o pré-processamento dos dados para encontrar a vazão (**throughput**) da rede com o padronizador *StandardScaler*.\n",
    "\n",
    "## Exercício 2\n",
    "No mesmo notebook do exercício 1, desfaça a padronização feita com o *StandardScaler* e repadronize os dados usando o *MinMaxScaler*.\n",
    "\n",
    "Obs.: Para fazer esse exercício, você deverá consultar a documentação das classes <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">StandardScaler</a> e <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
